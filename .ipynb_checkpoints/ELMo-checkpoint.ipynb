{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWa0-hU30o1k"
   },
   "source": [
    "# Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "2C2TtqNLzviH",
    "outputId": "81e27bdf-aabd-4481-8dcf-b1815443c511"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Embedding, LSTM, Conv1D, Conv2D, MaxPooling1D, Reshape, Flatten, Dropout, CuDNNLSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import operator\n",
    "\n",
    "\n",
    "# Import our dependencies\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "from keras import backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.engine import Layer\n",
    "\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#from appos import appos\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Start with loading all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WxTVgms1AGv"
   },
   "source": [
    "# Loading, extracting and pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "2V7SE2ER0_00",
    "outputId": "f7002251-f49e-439c-d368-8c15417cd224"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'file_path' with the actual path to your CSV file on your local system\n",
    "file_path = '/Users/teja/Downloads/Twitter_Data.csv'\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "data = data[['Text', 'Score']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Convert the ratings to the sentiments negative and positive.*\n",
    "Each sentiment will be represented by integers 0 or 1.\n",
    "\n",
    "0: negative\n",
    "\n",
    "1: positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1156
    },
    "colab_type": "code",
    "id": "3eSKUA_j0yUs",
    "outputId": "427430b9-8663-47ce-ac40-04b9e74129e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# negative reviews before: 90723\n",
      "# positive reviews before: 72250\n",
      "\n",
      "# negative reviews after: 72250\n",
      "# positive reviews after: 72250\n"
     ]
    }
   ],
   "source": [
    "# Assume 'data' is your DataFrame with 'Score' column containing 0 and 1 values\n",
    "\n",
    "# Define the mapping from 0, 1 ratings to negative or positive sentiment\n",
    "rating_to_sentiment = { 0: 0, 1: 1 }\n",
    "\n",
    "# Apply the mapping to create the 'Sentiment' column\n",
    "data['Sentiment'] = data['Score'].map(rating_to_sentiment)\n",
    "\n",
    "# Count number of negative and positive reviews\n",
    "neg_num = (data['Sentiment'] == 0).sum()\n",
    "pos_num = (data['Sentiment'] == 1).sum()\n",
    "\n",
    "print('# negative reviews before: {}'.format(neg_num))\n",
    "print('# positive reviews before: {}'.format(pos_num))\n",
    "\n",
    "# Make the dataset balanced\n",
    "balanced_sample_num = min(neg_num, pos_num)\n",
    "\n",
    "# Randomly select <'balanced_sample_num'> numbers of negative and positive reviews\n",
    "neg_sample = data[data['Sentiment'] == 0].sample(n=balanced_sample_num, replace=False)\n",
    "pos_sample = data[data['Sentiment'] == 1].sample(n=balanced_sample_num, replace=False)\n",
    "\n",
    "# Concatenate the samples to create the balanced dataset\n",
    "data_balanced = pd.concat([neg_sample, pos_sample], ignore_index=True)\n",
    "\n",
    "# Shuffle the rows so that 0's and 1's are mixed\n",
    "data_balanced = data_balanced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print('\\n# negative reviews after: {}'.format((data_balanced['Sentiment'] == 0).sum()))\n",
    "print('# positive reviews after: {}'.format((data_balanced['Sentiment'] == 1).sum()))\n",
    "\n",
    "# Get one-hot encoding for the labels\n",
    "Y = pd.get_dummies(data_balanced['Sentiment']).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mSMywl587BXt"
   },
   "source": [
    "# Perform pre-processing on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "id": "bni7aUYA6_5f",
    "outputId": "4f02cdca-6ab1-4d83-d143-8f97241edf55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/teja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/teja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Replace 'path_to_your_csv_file.csv' with the actual path to your CSV file on your local system\n",
    "file_path = '/Users/teja/Downloads/Twitter_Data.csv'\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "# All characters to lower case\n",
    "data['Text'] = data['Text'].apply(lambda x: x.lower() if isinstance(x, str) else str(x))\n",
    "\n",
    "# # Convert words with apostrophes to its corresponding words, e.g. \"it's\" -> \"it is\"\n",
    "# data['Text'] = data['Text'].apply(lambda x: x.split())\n",
    "# data['Text'] = data['Text'].apply(lambda x: \" \".join([appos[word] if word in appos else word for word in x]))\n",
    "\n",
    "# Remove html-tags, punctuation, commas, numbers etc\n",
    "data['Text'] = data['Text'].apply((lambda x: re.sub('<[^<]+?>', ' ', str(x))))\n",
    "data['Text'] = data['Text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', ' ', (x))))\n",
    "data['Text'] = data['Text'].apply((lambda x: re.sub('^\\d+\\s|\\s\\d+\\s|\\s\\d+$', ' ', (x))))\n",
    "\n",
    "# Convert text into tokens, in this case sentences into words\n",
    "data['Text'] = data.apply(lambda x: word_tokenize(x['Text']), axis = 1)\n",
    "\n",
    "# Remove most commonly occuring words which are not relevant in the context of the data\n",
    "irrelevant_words = stopwords.words('english')\n",
    "data['Text'] = data['Text'].apply(lambda x: [word for word in x if word not in irrelevant_words])\n",
    "\n",
    "# Find the base form of the word (lemmatization)\n",
    "lemma = WordNetLemmatizer()\n",
    "data['Text'] = data['Text'].apply(lambda x: \" \".join([lemma.lemmatize(word) for word in x]))\n",
    "\n",
    "# Vectorize the text by turning each review into a sequence of integers (each integer being the index of a token in a dictionary)\n",
    "# Also, pad so that every review has the same length\n",
    "#num_top_words = 10000\n",
    "#tokenizer = Tokenizer(num_words = num_top_words, split = ' ')\n",
    "#tokenizer.fit_on_texts(data['Text'].values)\n",
    "#X = tokenizer.texts_to_sequences(data['Text'].values)\n",
    "#X = pad_sequences(X)\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define the maximum sequence length\n",
    "max_length = 100  # Adjust as needed\n",
    "\n",
    "# Tokenize the text data\n",
    "X = []\n",
    "\n",
    "for text in data['Text'].values:\n",
    "    # Tokenize each text and truncate/pad to the specified maximum length\n",
    "    tokenized_text = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "    X.append(tokenized_text['input_ids'])\n",
    "\n",
    "# Convert the tokenized sequences to numpy array\n",
    "X = np.array(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IyQMOe5VnWC"
   },
   "source": [
    "Split data-set into 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFZABApc0x9f"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "kR744ZdqDaji",
    "outputId": "6f4abf54-f7a1-4aad-989b-2cb6a7608b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (70080, 3), Y train: (70080, 2)\n",
      "X test: (17520, 3), Y test: (17520, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"smaller_set_percentage = 0.5\\nsplit = int(round(smaller_set_percentage * data.shape[0]))\\n\\ntrain_x=data[split:]\\ntest_x=data[:split]\\ntrain_y=Y[split:]\\ntest_y=Y[:split]\\n\\nprint(train_x)\\nprint(test_y)\\nprint('training data set: X: {} Y: {}'.format(train_x.shape, train_y.shape))\\nprint('testing data set: X: {} Y: {}'.format(test_x.shape, test_y.shape))\""
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets with specified test and validation sizes\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "# Further split the train set into train and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=val_size)\n",
    "\n",
    "print('Train set: X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "print('Validation set: X: {}, Y: {}'.format(X_val.shape, Y_val.shape))\n",
    "print('Test set: X: {}, Y: {}'.format(X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mO83B6UEYHZz"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "\n",
    "# Define a custom layer for Elmo embeddings\n",
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024  # Dimensionality of Elmo embeddings\n",
    "        self.trainable=True  # Allow the layer to update weights\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Add the trainable weights of the Elmo module to the layer's weights\n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # Cast the input to string and remove extra dimensions\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                           as_dict=True,\n",
    "                           signature='default',\n",
    "                           )['default']\n",
    "        return result\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Compute mask to remove padding tokens\n",
    "        return K.not_equal(inputs, '--PAD--')\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Define the output shape of the layer\n",
    "        return (input_shape[0], self.dimensions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2043
    },
    "colab_type": "code",
    "id": "tXN8JDH7LHy0",
    "outputId": "60946947-ac5c-4a3e-df42-b2dcdcddad82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: allennlp in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
      "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.9.1)\n",
      "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8.1)\n",
      "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
      "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.5.3)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.6)\n",
      "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1)\n",
      "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.6.2)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.6)\n",
      "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.7)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
      "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.12.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.2.1)\n",
      "Requirement already satisfied: awscli>=1.11.91 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.160)\n",
      "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
      "Requirement already satisfied: spacy<2.2,>=2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0.18)\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
      "Requirement already satisfied: conllu==0.11 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.11)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
      "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
      "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.2)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
      "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.10.6)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.5.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.3)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.147)\n",
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.23)\n",
      "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.20.3)\n",
      "Requirement already satisfied: moto>=1.3.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
      "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
      "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.1)\n",
      "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2018.1.10)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\n",
      "Requirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
      "Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.4.2)\n",
      "Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.3.9)\n",
      "Collecting botocore==1.12.150 (from awscli>=1.11.91->allennlp)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a3/14582589522f21684726da7b0f57dc2258cc6e3adb6046675f3bd9eba834/botocore-1.12.150-py2.py3-none-any.whl (5.4MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4MB 32kB/s \n",
      "\u001b[?25hRequirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.2.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (2.0.1)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (0.2.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (1.0.2)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (1.35)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (6.12.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (2.0.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
      "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.15.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (41.0.1)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.0.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
      "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (0.12.0)\n",
      "Requirement already satisfied: boto>=2.36.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.49.0)\n",
      "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (3.0.5)\n",
      "Requirement already satisfied: cryptography>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.6.1)\n",
      "Requirement already satisfied: jsondiff==1.1.2 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (1.1.2)\n",
      "Requirement already satisfied: cfn-lint in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (0.20.3)\n",
      "Requirement already satisfied: python-jose<4.0.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (3.0.1)\n",
      "Requirement already satisfied: docker>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (3.7.2)\n",
      "Requirement already satisfied: aws-xray-sdk!=0.96,>=0.93 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.0)\n",
      "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.6.0)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.1)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.4.3.2)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (1.10.11)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.9.0.1)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (1.12.3)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (0.24.0)\n",
      "Requirement already satisfied: jsonschema~=2.6 in /usr/local/lib/python3.6/dist-packages (from cfn-lint->moto>=1.3.4->allennlp) (2.6.0)\n",
      "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.6/dist-packages (from cfn-lint->moto>=1.3.4->allennlp) (1.23)\n",
      "Requirement already satisfied: aws-sam-translator>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cfn-lint->moto>=1.3.4->allennlp) (1.11.0)\n",
      "Requirement already satisfied: ecdsa<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<4.0.0->moto>=1.3.4->allennlp) (0.13.2)\n",
      "Requirement already satisfied: future<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<4.0.0->moto>=1.3.4->allennlp) (0.16.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from docker>=2.5.1->moto>=1.3.4->allennlp) (0.4.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from docker>=2.5.1->moto>=1.3.4->allennlp) (0.56.0)\n",
      "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from aws-xray-sdk!=0.96,>=0.93->moto>=1.3.4->allennlp) (1.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.9.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3.0->moto>=1.3.4->allennlp) (2.19)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.6/dist-packages (from jsonpatch->cfn-lint->moto>=1.3.4->allennlp) (2.0)\n",
      "Installing collected packages: botocore\n",
      "  Found existing installation: botocore 1.12.147\n",
      "    Uninstalling botocore-1.12.147:\n",
      "      Successfully uninstalled botocore-1.12.147\n",
      "Successfully installed botocore-1.12.150\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "botocore"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install allennlp\n",
    "\n",
    "from allennlp.training.metrics.metric import Metric\n",
    "from allennlp.training.metrics import f1_measure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "TROZTNo-YJa5",
    "outputId": "e6d0088e-4424-4edf-bfb1-37a69fe21849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_metrics in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
      "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.12.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.2.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.16.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n"
     ]
    }
   ],
   "source": [
    "# Install keras_metrics package\n",
    "!pip install keras_metrics\n",
    "\n",
    "# Import necessary libraries\n",
    "import keras\n",
    "import keras_metrics\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "# Define a function to build the model\n",
    "def build_model(): \n",
    "    # Define input layer for text data\n",
    "    input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
    "    \n",
    "    # Embedding layer using ElmoEmbeddingLayer\n",
    "    embedding = ElmoEmbeddingLayer()(input_text)\n",
    "    \n",
    "    # Fully connected layer with 256 units and ReLU activation function\n",
    "    dense = layers.Dense(256, activation='relu')(embedding)\n",
    "    \n",
    "    # Output layer with 1 unit and sigmoid activation function for binary classification\n",
    "    pred = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    # Create the model with input and output layers\n",
    "    model = Model(inputs=[input_text], outputs=pred)\n",
    "\n",
    "    # Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "7aDfsnk_YVin",
    "outputId": "7cffc97a-aaff-499a-b548-d3dfd0868082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['thick strong likely adding soup rice']\n",
      " ['wonderful product dog really love one thing make product better chew box would one every day month']\n",
      " ['movie always popular different entertaining fast moving hour half film available bllu ray look better nothing outstanding definite improvement shapnress several character mainly michael keaton unique sometimes revolting title character beetlejuice always fascinating watch whole movie also get lot humor scary special effect odd see alec baldwin low key role 90 played type guy davis look act like well davis many time played nice people viewer like took four viewing finally appreciated catharine hara comedic talent movie favorite someone find absolutely hilarious messed wife mother family move haunted house inhabited baldwin davis keaton made name actor whacked robin williams type role although never really followed anything popular film winona rider cute teenage daughter get fun supporting role diverse people talk show host dick cavett singer robert goulet actor jeffrey jones good tim burton directed film surprised typical occult theme ghost like heaven hell strange existence touted dead people go ridiculous']\n",
      " ...\n",
      " ['huge peanut butter fan usually force eat work dont time cook saw planter sale tried know much better buy brand eating type nom nom']\n",
      " ['boyfriend cat eating food six year switched cat developed uti urinating house also gained ton weight review found online show cat owner similar problem cat getting sick eating food please use caution could make cat ill cat eat food always keep eye always thought male cat prone utis yet female cat sick cost vet bill feeding food']\n",
      " ['enabler two dog insatiable addiction greenies every night around 45 p start whining barking circling stop get greenie day consistent buyer greenies like know get expensive local chain pet store expect pay 99 per bag greenies oz teenie piece lead review amazon performance delivering doggie treat reasonable price currently 99 per bag buy bag get one free addition free shipping save time needle trip pet store buy bag getting per bag fantastic deal term snack senior version teenie greenies great taste great original based dog ability consume minute softer consistency one dog nearly year old year old dental care stressing teeth really important long amazon maintains deal continue purchase item directly']]\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset\n",
    "# Extract 'Text' column from train_x DataFrame and convert it to a list\n",
    "train_text = train_x['Text'].tolist()\n",
    "# Limit each text to 150 words by splitting and joining\n",
    "train_text = [' '.join(t.split()[0:150]) for t in train_text]\n",
    "# Convert train_text to a numpy array with an additional axis\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "# Extract 'Sentiment' column from train_x DataFrame and convert it to a list\n",
    "train_label = train_x['Sentiment'].tolist()\n",
    "\n",
    "# Create test dataset\n",
    "# Extract 'Text' column from test_x DataFrame and convert it to a list\n",
    "test_text = test_x['Text'].tolist()\n",
    "# Limit each text to 150 words by splitting and joining\n",
    "test_text = [' '.join(t.split()[0:150]) for t in test_text]\n",
    "# Convert test_text to a numpy array with an additional axis\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "# Extract 'Sentiment' column from test_x DataFrame and convert it to a list\n",
    "test_label = test_x['Sentiment'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "q_O_AyqzZoJf",
    "outputId": "28a8c0d9-369c-44b8-bb20-0f11a564fdd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 14:44:49.580165 140253575722880 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "elmo_embedding_layer_4 (Elmo (None, 1024)              4         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,661\n",
      "Trainable params: 262,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 56064 samples, validate on 14016 samples\n",
      "Epoch 1/4\n",
      "56064/56064 [==============================] - 1198s 21ms/step - loss: 0.4884 - acc: 0.7647 - val_loss: 0.4738 - val_acc: 0.7722\n",
      "Epoch 2/4\n",
      "56064/56064 [==============================] - 1191s 21ms/step - loss: 0.4461 - acc: 0.7882 - val_loss: 0.4247 - val_acc: 0.8049\n",
      "Epoch 3/4\n",
      "56064/56064 [==============================] - 1186s 21ms/step - loss: 0.4289 - acc: 0.7991 - val_loss: 0.4836 - val_acc: 0.7703\n",
      "Epoch 4/4\n",
      "56064/56064 [==============================] - 1190s 21ms/step - loss: 0.4153 - acc: 0.8075 - val_loss: 0.4425 - val_acc: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8eb0e49518>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and fit\n",
    "val_size=0.2\n",
    "model = build_model()\n",
    "model.fit(train_text, \n",
    "          train_label,\n",
    "          validation_split=val_size,\n",
    "          epochs=4,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfprDT91hYi_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "qCyjqnSNPVFV",
    "outputId": "a3eb2354-48d1-4f99-85ef-ae4e3b2f42ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1696/17520 [=>............................] - ETA: 6:24"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-edb45d768bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myhat_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# predict crisp classes for test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0myhat_probs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,1024,1068,44] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node elmo_embedding_layer_4/elmo_embedding_layer_4_module_apply_default/bilm/CNN/Conv2D_6}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node dense_8/Sigmoid}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(X_test['Text'], verbose=1)\n",
    "# predict crisp classes for test set\n",
    "labels = (yhat_probs > 0.5).astype(np.int)\n",
    "print(labels)\n",
    "\n",
    "#yhat_classes = np.argmax(yhat_probs,axis=1)\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLr8gPd1QaH0"
   },
   "outputs": [],
   "source": [
    "print(yhat_probs)\n",
    "print(Y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8R0inQuxbqdA"
   },
   "outputs": [],
   "source": [
    "\n",
    "#test_label1= np.array(test_label)\n",
    "#print(test_label1)\n",
    "#print(test_label1.shape)\n",
    "#Y_test_new=test_label1[:,1]\n",
    "\n",
    "print(\"elmo_model\")\n",
    "accuracy = accuracy_score(Y_test_new,labels)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test_new, labels)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test_new,labels)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(Y_test_new,labels)\n",
    "print('F1 score: %f \\n\\n' % f1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " Elmo_amazon200k.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
